{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1067707e",
   "metadata": {},
   "source": [
    "## Features of PySpark- Apache spark \n",
    "\n",
    "- Works with big data\n",
    "- faster than mapreduce \n",
    "- Runs in cluster mode, ie distributed mode\n",
    "- Reusability: Spark code can be used for batch-processing, joining streaming data against historical data as well as running ad-hoc queries on streaming state. \n",
    "- Dynamic nature:Sparkoffers over 80 high-level operators that make it easy to build parallel apps.\n",
    "- Advanced Analytics: for machine learning and graph processing libraries\n",
    "- In Memory Computing: Unlike Hadoop MapReduce, Apache Spark is capable of processing tasks in memory and it is not required to write back intermediate results to the disk. This is how it acheives high speed\n",
    "- Supporting Multiple languages\n",
    "- Integrated with Hadoop\n",
    "- Cost effective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab09f3",
   "metadata": {},
   "source": [
    "### we need to create a new enviroment when I work with pyspark \n",
    "\n",
    "- Setup the pip package manager: pip -h\n",
    "- Install the virtualenv package: pip install virtualenv\n",
    "- Create the virtual environment: virtualenv mypython, where mypython is name of virtual env \n",
    "- Activate the virtual environment: source mypython/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b318fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/melissavidiera/opt/anaconda3/lib/python3.8/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/melissavidiera/opt/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43575caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if we have downloaded and installed pyspark \n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e0715f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c475495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prevan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whisky</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coco</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prevan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name   Age\n",
       "0  Melissa   28\n",
       "1   Prevan    5\n",
       "2   Whisky    4\n",
       "3     Coco    1\n",
       "4   Prevan    6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('agedata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa0fdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start sparl we need to activate spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a8e7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark findspark.init() sc = pyspark.SparkContext(appName=\"Practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4386fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1937c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').config(\"...\").getOrCreate()\n",
    "#sqlContext = SQLContext(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5981d948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://melissas-air:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7801098dc0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #shows specifications of spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593a582",
   "metadata": {},
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"...\") \\\n",
    "    .getOrCreate()\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf72100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d885e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "#pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b678674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('agedata.csv') #read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a307f57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark # shows 2 cols. its taking a and b as defaut cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee1a2fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|  Name |Age|\n",
      "|Melissa| 28|\n",
      "| Prevan|  5|\n",
      "| Whisky|  4|\n",
      "|   Coco|  1|\n",
      "| Prevan|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see dataset\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d7f82b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name : string, Age: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here c0 and c1 are considered as column names. \n",
    "# in order to  make name and age as columns\n",
    "\n",
    "spark.read.option('header','true').csv('agedata.csv') # 1st row will be considered as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e1b4cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|  Name |Age|\n",
      "+-------+---+\n",
      "|Melissa| 28|\n",
      "| Prevan|  5|\n",
      "| Whisky|  4|\n",
      "|   Coco|  1|\n",
      "| Prevan|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('agedata.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a1ba0875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_pyspark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52821f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_pyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe3f047d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_pyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprintSchema\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5a170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
